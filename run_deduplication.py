"""
é¢„è®­ç»ƒæ•°æ®å»é‡è¿è¡Œè„šæœ¬
ä½¿ç”¨MinHashç®—æ³•å¯¹complete_results.jsonè¿›è¡Œå»é‡å¤„ç†
"""

import os
import sys
from utils.minhash_deduplicator import process_pretrain_data_with_minhash
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹é¢„è®­ç»ƒæ•°æ®å»é‡å¤„ç†")
    
    # é…ç½®å‚æ•°
    input_file = "data/TC0001251339_complete_results.json"
    output_dir = "pretrain_data_deduplicated"
    similarity_threshold = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œå¯è°ƒæ•´
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        logger.info("è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œå½“å‰å·¥ä½œç›®å½•ä¸‹çš„dataæ–‡ä»¶å¤¹ä¸­åº”è¯¥æœ‰TC0001251339_complete_results.jsonæ–‡ä»¶")
        return
    
    try:
        # ä½¿ç”¨MinHashç®—æ³•å¤„ç†æ•°æ®
        logger.info(f"è¾“å…¥æ–‡ä»¶: {input_file}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        
        processed_data = process_pretrain_data_with_minhash(
            input_file=input_file,
            output_dir=output_dir,
            similarity_threshold=similarity_threshold
        )
        
        # è¾“å‡ºå¤„ç†ç»“æœ
        logger.info("=" * 50)
        logger.info("é¢„è®­ç»ƒæ•°æ®å»é‡å¤„ç†å®Œæˆï¼")
        logger.info("=" * 50)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        formatted_data = processed_data["formatted"]
        logger.info(f"ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
        logger.info(f"  ğŸ“ çº¯æ–‡æœ¬æ•°æ®: {len(formatted_data['pure_texts'])} æ¡")
        logger.info(f"  â“ é—®ç­”å¯¹æ•°æ®: {len(formatted_data['qa_formatted'])} å¯¹")
        logger.info(f"  ï¿½ï¿½ï¸  ç»“æ„åŒ–æ•°æ®: {len(formatted_data['structured_texts'])} æ¡")
        
        # æ˜¾ç¤ºå»é‡ç»Ÿè®¡
        logger.info(f"\nğŸ“ˆ å»é‡ç»Ÿè®¡:")
        for data_type, items in processed_data["deduplicated"].items():
            original_count = len(processed_data["extracted"].get(data_type, []))
            deduplicated_count = len(items)
            removed_count = original_count - deduplicated_count
            if original_count > 0:
                removal_rate = (removed_count / original_count) * 100
                logger.info(f"  {data_type}: {original_count} -> {deduplicated_count} (ç§»é™¤ {removed_count} æ¡, {removal_rate:.1f}%)")
        
        logger.info(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        logger.info(f"  ï¿½ï¿½ çº¯æ–‡æœ¬: {output_dir}/pure_texts.txt")
        logger.info(f"  â“ é—®ç­”å¯¹: {output_dir}/qa_pairs.jsonl")
        logger.info(f"  ï¿½ï¿½ï¸  ç»“æ„åŒ–: {output_dir}/structured_texts.jsonl")
        logger.info(f"  ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {output_dir}/processing_stats.json")
        
        logger.info(f"\nâœ… å»é‡å¤„ç†æˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        import traceback
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return

if __name__ == "__main__":
    main()